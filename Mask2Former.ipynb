{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask2Former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\livia\\anaconda3\\envs\\oneformer\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Benötigte Bibliotheken importieren\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Modellvorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\livia\\anaconda3\\envs\\oneformer\\lib\\site-packages\\transformers\\utils\\deprecation.py:165: UserWarning: The following named arguments are not valid for `Mask2FormerImageProcessor.__init__` and were ignored: '_max_size'\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Mask2FormerForUniversalSegmentation-Modell sowie Image Processor laden\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-tiny-ade-semantic\")\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-tiny-ade-semantic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bild laden\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verarbeitung mit dem Prozessor\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modellvorhersage\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_queries_logits: Enthält die Wahrscheinlichkeiten für die vorhergesagten Klassen.\n",
    "\n",
    "# masks_queries_logits: Enthält die vorhergesagten Masken für jedes Pixel.\n",
    "\n",
    "class_queries_logits = outputs.class_queries_logits\n",
    "masks_queries_logits = outputs.masks_queries_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Nachbearbeitung der Vorhersage\n",
    "predicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualisiere das Originalbild und die Segmentierungskarte\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Image\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(predicted_semantic_map, cmap=\"jet\")\n",
    "plt.title(\"Predicted Segmentation\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Datenvorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\livia\\AppData\\Local\\Temp\\ipykernel_15696\\3561232687.py:10: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  transforms.Resize((512, 512), interpolation=Image.NEAREST),\n",
      "c:\\Users\\livia\\anaconda3\\envs\\oneformer\\lib\\site-packages\\torchvision\\transforms\\transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transformationen für Bildverarbeitung\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Transformationen für Maskenverarbeitung\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512), interpolation=Image.NEAREST),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def load_data(image_dir, mask_dir):\n",
    "    images = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg')])\n",
    "    masks = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if f.endswith('.png')])\n",
    "    return images, masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion für das Training des Modells\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, image_transform, mask_transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.image_transform = image_transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        mask = Image.open(self.mask_paths[idx])\n",
    "        return self.image_transform(image), self.mask_transform(mask).long()\n",
    "\n",
    "# Daten laden\n",
    "train_images, train_masks = load_data(\"train/images\", \"train/masks\")\n",
    "val_images, val_masks = load_data(\"val/images\", \"val/masks\")\n",
    "\n",
    "train_dataset = SegmentationDataset(train_images, train_masks, image_transform, mask_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Trainingsfunktion\n",
    "def train_model(model, train_loader, processor, epochs=5, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for images, masks in tqdm(train_loader):\n",
    "            images = images.to(device)\n",
    "            masks = masks.squeeze(1).to(device)  # Binarisierte Labels\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Vorwärtsdurchlauf\n",
    "            inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(pixel_values=inputs[\"pixel_values\"], labels={\"mask_labels\": masks})\n",
    "\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oneformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
